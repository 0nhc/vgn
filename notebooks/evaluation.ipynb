{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGN Evaluation\n",
    "\n",
    "Compute test metrics, plot learning curves and run simulated cutter removal experiments with a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from pathlib2 import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rospy\n",
    "\n",
    "from vgn import benchmark\n",
    "from vgn.dataset import Dataset\n",
    "from vgn_ros import vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNamespace(object):\n",
    "    def __init__ (self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node(\"vgn_evaluation\", anonymous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Loss and accuracy on test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Metrics and failure cases of a clutter removal experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path(\"data/experiments/meh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate, percent_cleared, planning_time = benchmark.metrics(logdir)\n",
    "\n",
    "print(\"Success rate   \", success_rate)\n",
    "print(\"Percent cleared\", percent_cleared)\n",
    "print(\"Planning time  \", planning_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize failure cases in rviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(logdir)\n",
    "failures = dataset.df[dataset.df[\"label\"] == 0].index.tolist()\n",
    "iterator = iter(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = next(iterator)\n",
    "dataset.draw(i, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "Loss and grasp metrics vs training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = Path(\"data/runs/200610-2009,dataset=train,augment=True,net=conv,batch_size=32,lr=3e-04\")\n",
    "exp_name = \"06_train_augment\"\n",
    "object_set = \"adversarial\"\n",
    "epochs_to_evaluate = [5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run clutter removal experiment for each `epochs_to_evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs_to_evaluate:\n",
    "    model = run_dir / (\"vgn_conv_\" + str(epoch) + \".pth\")\n",
    "    logdir = Path(\"data\") / \"experiments\" / exp_name / object_set / str(epoch)    \n",
    "    \n",
    "    if logdir.exists():\n",
    "        continue  # manually delete folder to rerun benchmark\n",
    "    \n",
    "    args = SimpleNamespace(\n",
    "        model=model,\n",
    "        logdir=logdir,\n",
    "        rounds=40,\n",
    "        object_set=object_set,\n",
    "        object_count=5,\n",
    "        seed=1,\n",
    "        sim_gui=False,\n",
    "        rviz=False,\n",
    "    )\n",
    "    benchmark.main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train and validation losses (csvs need to be downloaded from TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(run_dir / \"train\" / \"loss.csv\")\n",
    "train_epochs = df[\"Step\"].to_numpy()\n",
    "train_loss = df[\"Value\"].to_numpy()\n",
    "\n",
    "df = pd.read_csv(run_dir / \"validation\" / \"loss.csv\")\n",
    "val_epochs = df[\"Step\"].to_numpy()\n",
    "val_loss = df[\"Value\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the benchmark metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"data\") / \"experiments\" / exp_name / object_set\n",
    "success_rates = []\n",
    "for epoch in epochs_to_evaluate:\n",
    "    log_dir = root / str(epoch)\n",
    "    success_rate, _, _ = benchmark.metrics(log_dir)\n",
    "    success_rates.append(success_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "l1, = ax1.plot(train_epochs, train_loss, color=\"C0\")\n",
    "l2,= ax1.plot(val_epochs, val_loss, color=\"C1\")\n",
    "l3, = ax2.plot(epochs_to_evaluate, success_rates, color=\"C2\")\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.set_ylabel(\"%\")\n",
    "\n",
    "ax1.legend([l1, l2], [\"train\", \"validation\"], loc=\"lower left\")\n",
    "ax2.legend([l3], [\"success rate\"], loc=\"upper right\")\n",
    "\n",
    "fig.suptitle(exp_name)\n",
    "fig_path = Path.home() / \"Desktop\" / (exp_name + \".png\")\n",
    "plt.savefig(str(fig_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
